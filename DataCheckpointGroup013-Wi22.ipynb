{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COGS 108 - Data Checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Names\n",
    "\n",
    "- Tyler Le\n",
    "- Aditya Tomar\n",
    "- William Lynch\n",
    "- Michael Mao\n",
    "- Natalie Quach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='research_question'></a>\n",
    "# Research Question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is there a positive correlation between the cost of living and the impact of natural disasters in terms of injuries, casualties, and property damage per capita at the county level? Furthermore, in which state does the impact of natural disasters affect cost of living the most?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset(s)\n",
    "Please note that all data file names correspond to their name in our \"datasets\" folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Frequency of Disasters By State**\n",
    "- Dataset Name: Billion-Dollar Disasters By Year (CPI-Adjusted) (natural_disaster_frequencies.csv)\n",
    "- Link to the dataset: https://www.ncdc.noaa.gov/billions/state-freq-data.csv\n",
    "- Number of observations: 2228 \n",
    "- Number of features: 9. These features are mostly numerical.\n",
    "\n",
    "This dataset contains the count of natural disasters that cost more than 1 billion dollars for each state from 1980 - 2021. Each observation contains one state and the count of each natural disaster type for a given year.\n",
    "\n",
    "**2. Types of Disaster By State/County**\n",
    "- Dataset Name: Disaster Declarations Summaries (DisasterDeclarationsSummaries.csv)\n",
    "- Link to the dataset: https://www.fema.gov/api/open/v1/DisasterDeclarationsSummaries.csv\n",
    "- Number of observations: 62771\n",
    "- Number of features: 23. Contains a combination of numerical and string data types.\n",
    "\n",
    "This dataset contains all federally declared natural disasters from 1953-2022 (by year, state, county, and type) along with declared recovery programs. Each observation contains the type of natural disaster, when it occurred, and the state/county it occured in.\n",
    "\n",
    "\n",
    "**3. National Risk Index (NRI)**\n",
    "- Dataset Name: National Risk Index per County (NRI_Table_Counties.csv)\n",
    "- Link to the dataset: https://hazards.fema.gov/nri/data-resources#csvDownload\n",
    "- Number of observations: 3142\n",
    "- Number of features: 365. Contains a combination of numerical and string data types.\n",
    "\n",
    "Dataset from FEMA that identifies counties and states most at risk to 18 natural hazards. Includes data about expected annual losses from natural hazards, social vulnerability and community resilience. \n",
    "\n",
    "**4. States with Coastline**\n",
    "- Dataset Name: States with Coastline (states_with_coastline.csv)\n",
    "- Link to the dataset: https://worldpopulationreview.com/state-rankings/coastal-states\n",
    "- Number of observations: 50\n",
    "- Number of features: 2. Contains string data types.\n",
    "\n",
    "This dataset contains whether or not each state in the United States has a coast. Each observation contains a state and its associated coast.\n",
    "\n",
    "\n",
    "**5. Cost of Living**\n",
    "- Dataset Name: Cost of Living (cost_of_living.csv)\n",
    "- Link to the dataset: https://advisorsmith.com/wp-content/uploads/2021/02/advisorsmith_cost_of_living_index.csv\n",
    "- Number of observations: 510\n",
    "- Number of features: 3. Contains numerical and string data types.\n",
    "\n",
    "Each observation in this dataset contains a state, the city associated with the state, and the Cost of Living Index. The Cost of Living Index measures the costs such as food and energy.\n",
    "\n",
    "**6. Climate**\n",
    "- Dataset Name: Average Climate by County (Average_Climate_By_County.csv)\n",
    "- Link to the dataset: https://www.ncdc.noaa.gov/cag/county/mapping\n",
    "- Number of observations: 3137\n",
    "- Number of features: 3. Contains a combination of numerical and string types.\n",
    "\n",
    "This dataset contains the mean climate (measured in Fahrenheit) over a 5-year span from 2017 to 2022 for all counties in the USA except those in Hawaii. This dataset is for comparing the correlation between natural disasters vs cost of living with climate vs cost of living because climate is a potential confounding variable that affects cost of living.\n",
    "\n",
    "**6. Housing Price**\n",
    "- Dataset Name: \n",
    "- Link to the dataset: \n",
    "- Number of observations: \n",
    "- Number of features: \n",
    "\n",
    "This dataset contains ...\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from geopy.geocoders import Nominatim\n",
    "from geopy.extra.rate_limiter import RateLimiter\n",
    "from tqdm import tqdm # progress bar for .apply()\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table #1 (Frequency of Disaster By State):** This dataset was fairly clean in that there were no missing values and each observation contains whether or not a state had a natural disaster event in a certain year. We decided to remove spaces from the column names and to replace them with underscores. Since each column describing a natural disaster contains the count of how many natural disasters of that type, there was little data cleaning needed for this dataset.\n",
    "\n",
    "**Table #2 (Types of Disasters By State/County):** This dataset was fairly clean. We first focused on extracting the relevant columns, which were \"state\", \"declarationType\", \"incidentType\", \"declarationTitle\", and \"declarationArea\". These were the relevant variables since we want each state, the type of natural disaster, and whether it occured on a county level or not. We filtered the dataset to only contain natural disasters that occurred at the county level and standardized the column. We decided to keep the year it happened rather than the exact month and day since in our EDA in the future we would like to explore the natural disaster frequencies by decade. To make future analyses more convienient, we renamed some of the column names. Also, we checked for missing values and found that there were none. \n",
    "\n",
    "**Table #3 (National Risk Index):** This dataset was fairly clean. We focused on extracting the relevant columns, such as county, population size, National Risk Index score, and expected annual loss. These variables are important because we would like to compare counties per capita. We also decided to lowercase all the columns and replace spaces with underscores for consistency across all dataframes.\n",
    "\n",
    "**Table #4 (States with Coastline):** This dataset was fairly clean. Originally, each observation in this dataset contained a state and its associated coast. If the state did not have a coast associated, it had a value \"None\". To aid with future analyses, such as fitting multiple linear regression models later on, we changed the \"coast\" column to be binary where 0 means that a state does not have a coast associated with it and 1 means that a state does have a coast associated with it.\n",
    "\n",
    "**Table #5 (Cost of Living):** This dataset was fairly clean. However, we wanted to look at cost of living as it relates to injuries, casualties, and private property damage broken up by county. Currently, the data we have only shows the cities. We used a geocoder to retrieve the county information. We then made a new column in the dataframe to store counties for each city and rearranged the columns to a favored format.\n",
    "\n",
    "**Table #6 (Average Climate by County)**\n",
    "This dataset was very clean. All that we needed to change was remove ID numbers after the state abbreviations and change the column names from \"Location ID\" to \"State\", \"Location\" to \"County\", and \"Value\" to \"Temperature (F)\". \n",
    "\n",
    "We plan to merge each of the datasets by either the county or state columns. Additionally, we have datasets containing the number of casualties, number of injuries, and property damage amount per state for the years 2015-2020 that we plan on processing and cleaning after the checkpoint (these datasets are in the \"datasets/nat_disast_bystate_deaths_cost” folder).\n",
    "\n",
    "**Table #7 (Housing Price)**\n",
    "FILL OUT LATER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Table #1 (Frequency of Disaster By State)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>state</th>\n",
       "      <th>drought</th>\n",
       "      <th>flooding</th>\n",
       "      <th>freeze</th>\n",
       "      <th>severe_storm</th>\n",
       "      <th>tropical_cyclone</th>\n",
       "      <th>wildfire</th>\n",
       "      <th>winter_storm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1980</td>\n",
       "      <td>AK</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1980</td>\n",
       "      <td>AL</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1980</td>\n",
       "      <td>AR</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1980</td>\n",
       "      <td>AZ</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1980</td>\n",
       "      <td>CA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2221</th>\n",
       "      <td>2021</td>\n",
       "      <td>VT</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2222</th>\n",
       "      <td>2021</td>\n",
       "      <td>WA</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2223</th>\n",
       "      <td>2021</td>\n",
       "      <td>WI</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2224</th>\n",
       "      <td>2021</td>\n",
       "      <td>WV</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2225</th>\n",
       "      <td>2021</td>\n",
       "      <td>WY</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2226 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      year state  drought  flooding  freeze  severe_storm  tropical_cyclone  \\\n",
       "0     1980    AK        0         0       0             0                 0   \n",
       "1     1980    AL        1         0       0             0                 0   \n",
       "2     1980    AR        1         1       0             0                 0   \n",
       "3     1980    AZ        0         0       0             0                 0   \n",
       "4     1980    CA        0         0       0             0                 0   \n",
       "...    ...   ...      ...       ...     ...           ...               ...   \n",
       "2221  2021    VT        0         0       0             0                 0   \n",
       "2222  2021    WA        1         0       0             0                 0   \n",
       "2223  2021    WI        0         0       0             4                 0   \n",
       "2224  2021    WV        0         0       0             0                 1   \n",
       "2225  2021    WY        1         0       0             0                 0   \n",
       "\n",
       "      wildfire  winter_storm  \n",
       "0            0             0  \n",
       "1            0             0  \n",
       "2            0             0  \n",
       "3            0             0  \n",
       "4            0             0  \n",
       "...        ...           ...  \n",
       "2221         0             0  \n",
       "2222         1             1  \n",
       "2223         0             0  \n",
       "2224         0             0  \n",
       "2225         1             0  \n",
       "\n",
       "[2226 rows x 9 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_df = pd.read_csv('datasets/natural_disaster_frequencies.csv')\n",
    "\n",
    "# replace space with underscores in column names\n",
    "freq_df.columns = freq_df.columns.str.replace(' ', '_')\n",
    "\n",
    "# check for NaNs\n",
    "assert(freq_df.isna().sum().sum() == 0)\n",
    "\n",
    "freq_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Table #2 (Types of Disaster By State/County)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_county(str_in):\n",
    "    try:\n",
    "        if '(County)' in str_in:\n",
    "            output = str_in.replace('(County)','')\n",
    "        else:\n",
    "            output = None\n",
    "    except: \n",
    "        output = None\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "def standardize_year(str_in):\n",
    "    try:\n",
    "        output = str_in.split('T')[0]\n",
    "        output = pd.to_datetime(str_in).year\n",
    "    except:\n",
    "        output = None\n",
    "        \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state</th>\n",
       "      <th>year</th>\n",
       "      <th>disaster_type</th>\n",
       "      <th>disaster_declaration</th>\n",
       "      <th>county</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>IN</td>\n",
       "      <td>1959</td>\n",
       "      <td>Flood</td>\n",
       "      <td>FLOOD</td>\n",
       "      <td>Clay</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>WA</td>\n",
       "      <td>1964</td>\n",
       "      <td>Flood</td>\n",
       "      <td>HEAVY RAINS &amp; FLOODING</td>\n",
       "      <td>Wahkiakum</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>WA</td>\n",
       "      <td>1964</td>\n",
       "      <td>Flood</td>\n",
       "      <td>HEAVY RAINS &amp; FLOODING</td>\n",
       "      <td>Skamania</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>WA</td>\n",
       "      <td>1964</td>\n",
       "      <td>Flood</td>\n",
       "      <td>HEAVY RAINS &amp; FLOODING</td>\n",
       "      <td>Pierce</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>WA</td>\n",
       "      <td>1964</td>\n",
       "      <td>Flood</td>\n",
       "      <td>HEAVY RAINS &amp; FLOODING</td>\n",
       "      <td>Pacific</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55040</th>\n",
       "      <td>WA</td>\n",
       "      <td>2022</td>\n",
       "      <td>Flood</td>\n",
       "      <td>SEVERE STORMS, STRAIGHT-LINE WINDS, FLOODING, ...</td>\n",
       "      <td>Whatcom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55041</th>\n",
       "      <td>WA</td>\n",
       "      <td>2022</td>\n",
       "      <td>Flood</td>\n",
       "      <td>SEVERE STORMS, STRAIGHT-LINE WINDS, FLOODING, ...</td>\n",
       "      <td>Clallam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55042</th>\n",
       "      <td>WA</td>\n",
       "      <td>2022</td>\n",
       "      <td>Flood</td>\n",
       "      <td>SEVERE STORMS, STRAIGHT-LINE WINDS, FLOODING, ...</td>\n",
       "      <td>Skagit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55043</th>\n",
       "      <td>TN</td>\n",
       "      <td>2022</td>\n",
       "      <td>Tornado</td>\n",
       "      <td>SEVERE STORMS, STRAIGHT-LINE WINDS, AND TORNADOES</td>\n",
       "      <td>Gibson</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55044</th>\n",
       "      <td>TN</td>\n",
       "      <td>2022</td>\n",
       "      <td>Tornado</td>\n",
       "      <td>SEVERE STORMS, STRAIGHT-LINE WINDS, AND TORNADOES</td>\n",
       "      <td>Dyer</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>55045 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      state  year disaster_type  \\\n",
       "0        IN  1959         Flood   \n",
       "1        WA  1964         Flood   \n",
       "2        WA  1964         Flood   \n",
       "3        WA  1964         Flood   \n",
       "4        WA  1964         Flood   \n",
       "...     ...   ...           ...   \n",
       "55040    WA  2022         Flood   \n",
       "55041    WA  2022         Flood   \n",
       "55042    WA  2022         Flood   \n",
       "55043    TN  2022       Tornado   \n",
       "55044    TN  2022       Tornado   \n",
       "\n",
       "                                    disaster_declaration      county  \n",
       "0                                                  FLOOD       Clay   \n",
       "1                                 HEAVY RAINS & FLOODING  Wahkiakum   \n",
       "2                                 HEAVY RAINS & FLOODING   Skamania   \n",
       "3                                 HEAVY RAINS & FLOODING     Pierce   \n",
       "4                                 HEAVY RAINS & FLOODING    Pacific   \n",
       "...                                                  ...         ...  \n",
       "55040  SEVERE STORMS, STRAIGHT-LINE WINDS, FLOODING, ...    Whatcom   \n",
       "55041  SEVERE STORMS, STRAIGHT-LINE WINDS, FLOODING, ...    Clallam   \n",
       "55042  SEVERE STORMS, STRAIGHT-LINE WINDS, FLOODING, ...     Skagit   \n",
       "55043  SEVERE STORMS, STRAIGHT-LINE WINDS, AND TORNADOES     Gibson   \n",
       "55044  SEVERE STORMS, STRAIGHT-LINE WINDS, AND TORNADOES       Dyer   \n",
       "\n",
       "[55045 rows x 5 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "disaster_type_df = pd.read_csv('datasets/DisasterDeclarationsSummaries.csv')\n",
    "\n",
    "# select a subset of the columns\n",
    "wanted_columns = ['state', 'declarationDate','incidentType','declarationTitle','designatedArea']\n",
    "\n",
    "# rename the columns\n",
    "disaster_type_df = disaster_type_df[wanted_columns].rename(columns={\"declarationDate\":\"year\", \"designatedArea\": \"county\", \"incidentType\":\"disaster_type\", \"declarationTitle\":\"disaster_declaration\"})\n",
    "\n",
    "# Set \"Statewide\" to None and strip \"(County)\" from all counties\n",
    "disaster_type_df['county'] = disaster_type_df['county'].apply(standardize_county)\n",
    "\n",
    "# filter dataset to only include non-null \n",
    "disaster_type_df = disaster_type_df[~disaster_type_df['county'].isnull()]\n",
    "\n",
    "# strip year column to only include year\n",
    "disaster_type_df['year'] = disaster_type_df['year'].apply(standardize_year)\n",
    "\n",
    "# sort by year\n",
    "disaster_type_df = disaster_type_df.sort_values('year').reset_index(drop = True)\n",
    "\n",
    "# check for no NaNs\n",
    "assert(disaster_type_df.isna().sum().sum() == 0)\n",
    "\n",
    "disaster_type_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Table #3 (NRI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the \"NRI Data Dictionary in the datasets/NRI_Table_Counties\" to see what the cols mean\n",
    "# EAL = \"Expected Annual Lost\", quantifies the anticipated economic damage resulting from natural hazards each year. \n",
    "# 1-100 scale\n",
    "\n",
    "df_nri = pd.read_csv('datasets/NRI_Table_Counties/NRI_Table_Counties.csv')\n",
    "\n",
    "# select a subset of the columns\n",
    "wanted_cols = ['STATE','STATEABBRV','STATEFIPS','COUNTY','COUNTYFIPS','POPULATION','AREA','RISK_SCORE','RISK_RATNG','EAL_SCORE','EAL_RATNG']\n",
    "df_nri = df_nri[wanted_cols]\n",
    "\n",
    "# lowercase all columns\n",
    "df_nri.columns = df_nri.columns.str.lower()\n",
    "\n",
    "# rename columns\n",
    "df_nri = df_nri.rename(columns={\"stateabbrv\":\"state_abbrv\", \"risk_ratng\":\"risk_rating\" ,\"eal_ratng\":\"eal_rating\",\"statefips\":\"state_fips\",\"countyfips\":\"county_fips\"})\n",
    "\n",
    "# we need state and county FIPS information (already included in NRI dataset)\n",
    "df_nri['state_fips'] = df_nri['state_fips'].apply(lambda x: str(x).zfill(2))\n",
    "df_nri['county_fips'] = df_nri['county_fips'].apply(lambda x: str(x).zfill(3))\n",
    "df_nri['fips'] = df_nri['state_fips'] + df_nri['county_fips']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to convert risk/eal rating to values.\n",
    "df_nri.value_counts('risk_rating')\n",
    "encoding_dict = {\"Very Low\": 1, \"Relatively Low\":2, \"Relatively Moderate\": 3, \"Relatively High\": 4, \"Very High\": 5}\n",
    "\n",
    "def encode_ratings(str_in):\n",
    "    return (encoding_dict[str_in])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nri['risk_encoded'] = df_nri['risk_rating'].apply(encode_ratings)\n",
    "df_nri['eal_encoded'] = df_nri['eal_rating'].apply(encode_ratings)\n",
    "\n",
    "# make sure there are no NaNs\n",
    "assert(df_nri.isna().sum().sum() == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nri.to_csv(\"datasets/cleaned/nri.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state</th>\n",
       "      <th>state_abbrv</th>\n",
       "      <th>state_fips</th>\n",
       "      <th>county</th>\n",
       "      <th>county_fips</th>\n",
       "      <th>population</th>\n",
       "      <th>area</th>\n",
       "      <th>risk_score</th>\n",
       "      <th>risk_rating</th>\n",
       "      <th>eal_score</th>\n",
       "      <th>eal_rating</th>\n",
       "      <th>fips</th>\n",
       "      <th>risk_encoded</th>\n",
       "      <th>eal_encoded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Kentucky</td>\n",
       "      <td>KY</td>\n",
       "      <td>21</td>\n",
       "      <td>Johnson</td>\n",
       "      <td>115</td>\n",
       "      <td>23356</td>\n",
       "      <td>261.958144</td>\n",
       "      <td>9.281419</td>\n",
       "      <td>Relatively Low</td>\n",
       "      <td>11.921944</td>\n",
       "      <td>Relatively Low</td>\n",
       "      <td>21115</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Kentucky</td>\n",
       "      <td>KY</td>\n",
       "      <td>21</td>\n",
       "      <td>Kenton</td>\n",
       "      <td>117</td>\n",
       "      <td>159720</td>\n",
       "      <td>160.213975</td>\n",
       "      <td>10.449057</td>\n",
       "      <td>Relatively Low</td>\n",
       "      <td>16.837131</td>\n",
       "      <td>Relatively Moderate</td>\n",
       "      <td>21117</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Kentucky</td>\n",
       "      <td>KY</td>\n",
       "      <td>21</td>\n",
       "      <td>Knott</td>\n",
       "      <td>119</td>\n",
       "      <td>16346</td>\n",
       "      <td>351.517978</td>\n",
       "      <td>10.068395</td>\n",
       "      <td>Relatively Low</td>\n",
       "      <td>10.945913</td>\n",
       "      <td>Relatively Low</td>\n",
       "      <td>21119</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Kentucky</td>\n",
       "      <td>KY</td>\n",
       "      <td>21</td>\n",
       "      <td>Knox</td>\n",
       "      <td>121</td>\n",
       "      <td>31883</td>\n",
       "      <td>386.298435</td>\n",
       "      <td>11.858245</td>\n",
       "      <td>Relatively Low</td>\n",
       "      <td>11.983719</td>\n",
       "      <td>Relatively Low</td>\n",
       "      <td>21121</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Kentucky</td>\n",
       "      <td>KY</td>\n",
       "      <td>21</td>\n",
       "      <td>Larue</td>\n",
       "      <td>123</td>\n",
       "      <td>14193</td>\n",
       "      <td>261.539564</td>\n",
       "      <td>4.610900</td>\n",
       "      <td>Very Low</td>\n",
       "      <td>7.028611</td>\n",
       "      <td>Very Low</td>\n",
       "      <td>21123</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      state state_abbrv state_fips   county county_fips  population  \\\n",
       "0  Kentucky          KY         21  Johnson         115       23356   \n",
       "1  Kentucky          KY         21   Kenton         117      159720   \n",
       "2  Kentucky          KY         21    Knott         119       16346   \n",
       "3  Kentucky          KY         21     Knox         121       31883   \n",
       "4  Kentucky          KY         21    Larue         123       14193   \n",
       "\n",
       "         area  risk_score     risk_rating  eal_score           eal_rating  \\\n",
       "0  261.958144    9.281419  Relatively Low  11.921944       Relatively Low   \n",
       "1  160.213975   10.449057  Relatively Low  16.837131  Relatively Moderate   \n",
       "2  351.517978   10.068395  Relatively Low  10.945913       Relatively Low   \n",
       "3  386.298435   11.858245  Relatively Low  11.983719       Relatively Low   \n",
       "4  261.539564    4.610900        Very Low   7.028611             Very Low   \n",
       "\n",
       "    fips  risk_encoded  eal_encoded  \n",
       "0  21115             2            2  \n",
       "1  21117             2            3  \n",
       "2  21119             2            2  \n",
       "3  21121             2            2  \n",
       "4  21123             1            1  "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_nri.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Table #4 (States with Coastline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>State</th>\n",
       "      <th>coast</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>Gulf Coast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Alaska</td>\n",
       "      <td>Pacific Ocean</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Arizona</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Arkansas</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>California</td>\n",
       "      <td>Pacific Ocean</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Colorado</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Connecticut</td>\n",
       "      <td>Atlantic Ocean</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Delaware</td>\n",
       "      <td>Atlantic Ocean</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Florida</td>\n",
       "      <td>Atlantic Ocean/Gulf Coast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Georgia</td>\n",
       "      <td>Atlantic Ocean</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Hawaii</td>\n",
       "      <td>Pacific Ocean</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Idaho</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Illinois</td>\n",
       "      <td>Great Lakes Coast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Indiana</td>\n",
       "      <td>Great Lakes Coast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Iowa</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Kansas</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Kentucky</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Louisiana</td>\n",
       "      <td>Gulf Coast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Maine</td>\n",
       "      <td>Atlantic Ocean</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Maryland</td>\n",
       "      <td>Atlantic Ocean</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Massachusetts</td>\n",
       "      <td>Atlantic Ocean</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Michigan</td>\n",
       "      <td>Great Lakes Coast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Minnesota</td>\n",
       "      <td>Great Lakes Coast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Mississippi</td>\n",
       "      <td>Gulf Coast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Missouri</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Montana</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Nebraska</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Nevada</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>New Hampshire</td>\n",
       "      <td>Atlantic Ocean</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>New Jersey</td>\n",
       "      <td>Atlantic Ocean</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>New Mexico</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>New York</td>\n",
       "      <td>Atlantic Ocean/Great Lakes Coast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>North Carolina</td>\n",
       "      <td>Atlantic Ocean</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>North Dakota</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Ohio</td>\n",
       "      <td>Great Lakes Coast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Oklahoma</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Oregon</td>\n",
       "      <td>Pacific Ocean</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Pennsylvania</td>\n",
       "      <td>Great Lakes Coast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Rhode Island</td>\n",
       "      <td>Atlantic Ocean</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>South Carolina</td>\n",
       "      <td>Atlantic Ocean</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>South Dakota</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>Tennessee</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>Texas</td>\n",
       "      <td>Gulf Coast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>Utah</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>Vermont</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>Virginia</td>\n",
       "      <td>Atlantic Ocean</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>Washington</td>\n",
       "      <td>Pacific Ocean</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>West Virginia</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>Wisconsin</td>\n",
       "      <td>Great Lakes Coast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>Wyoming</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             State                             coast\n",
       "0          Alabama                        Gulf Coast\n",
       "1           Alaska                     Pacific Ocean\n",
       "2          Arizona                              None\n",
       "3         Arkansas                              None\n",
       "4       California                     Pacific Ocean\n",
       "5         Colorado                              None\n",
       "6      Connecticut                    Atlantic Ocean\n",
       "7         Delaware                    Atlantic Ocean\n",
       "8          Florida         Atlantic Ocean/Gulf Coast\n",
       "9          Georgia                    Atlantic Ocean\n",
       "10          Hawaii                     Pacific Ocean\n",
       "11           Idaho                              None\n",
       "12        Illinois                 Great Lakes Coast\n",
       "13         Indiana                 Great Lakes Coast\n",
       "14            Iowa                              None\n",
       "15          Kansas                              None\n",
       "16        Kentucky                              None\n",
       "17       Louisiana                        Gulf Coast\n",
       "18           Maine                    Atlantic Ocean\n",
       "19        Maryland                    Atlantic Ocean\n",
       "20   Massachusetts                    Atlantic Ocean\n",
       "21        Michigan                 Great Lakes Coast\n",
       "22       Minnesota                 Great Lakes Coast\n",
       "23     Mississippi                        Gulf Coast\n",
       "24        Missouri                              None\n",
       "25         Montana                              None\n",
       "26        Nebraska                              None\n",
       "27          Nevada                              None\n",
       "28   New Hampshire                    Atlantic Ocean\n",
       "29      New Jersey                    Atlantic Ocean\n",
       "30      New Mexico                              None\n",
       "31        New York  Atlantic Ocean/Great Lakes Coast\n",
       "32  North Carolina                    Atlantic Ocean\n",
       "33    North Dakota                              None\n",
       "34            Ohio                 Great Lakes Coast\n",
       "35        Oklahoma                              None\n",
       "36          Oregon                     Pacific Ocean\n",
       "37    Pennsylvania                 Great Lakes Coast\n",
       "38    Rhode Island                    Atlantic Ocean\n",
       "39  South Carolina                    Atlantic Ocean\n",
       "40    South Dakota                              None\n",
       "41       Tennessee                              None\n",
       "42           Texas                        Gulf Coast\n",
       "43            Utah                              None\n",
       "44         Vermont                              None\n",
       "45        Virginia                    Atlantic Ocean\n",
       "46      Washington                     Pacific Ocean\n",
       "47   West Virginia                              None\n",
       "48       Wisconsin                 Great Lakes Coast\n",
       "49         Wyoming                              None"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coastline_df = pd.read_csv('datasets/states_with_coastline.csv')\n",
    "coastline_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "State    object\n",
       "coast    object\n",
       "dtype: object"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check data types for each column\n",
    "coastline_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "None                                20\n",
       "Atlantic Ocean                      12\n",
       "Great Lakes Coast                    7\n",
       "Pacific Ocean                        5\n",
       "Gulf Coast                           4\n",
       "Atlantic Ocean/Gulf Coast            1\n",
       "Atlantic Ocean/Great Lakes Coast     1\n",
       "Name: coast, dtype: int64"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coastline_df['coast'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "State    0\n",
       "coast    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check for missing values\n",
    "coastline_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binarize \"coast\" column. 0 = no coastline, 1 = yes coastline\n",
    "def clean_coast(coast_val):\n",
    "    coast_val = coast_val.lower()\n",
    "\n",
    "    if \"none\" in coast_val:\n",
    "        coast_val = coast_val.replace(\"none\", \"0\")\n",
    "        output = int(coast_val)\n",
    "    else:\n",
    "        output = 1\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test function from above\n",
    "assert clean_coast('None') == 0\n",
    "assert clean_coast('Atlantic Ocean') == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "coastline_df['coast'] = coastline_df['coast'].apply(clean_coast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>State</th>\n",
       "      <th>coast</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Alaska</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Arizona</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Arkansas</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>California</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Colorado</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Connecticut</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Delaware</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Florida</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Georgia</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Hawaii</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Idaho</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Illinois</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Indiana</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Iowa</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Kansas</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Kentucky</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Louisiana</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Maine</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Maryland</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Massachusetts</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Michigan</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Minnesota</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Mississippi</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Missouri</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Montana</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Nebraska</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Nevada</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>New Hampshire</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>New Jersey</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>New Mexico</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>New York</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>North Carolina</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>North Dakota</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Ohio</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Oklahoma</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Oregon</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Pennsylvania</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Rhode Island</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>South Carolina</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>South Dakota</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>Tennessee</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>Texas</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>Utah</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>Vermont</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>Virginia</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>Washington</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>West Virginia</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>Wisconsin</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>Wyoming</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             State  coast\n",
       "0          Alabama      1\n",
       "1           Alaska      1\n",
       "2          Arizona      0\n",
       "3         Arkansas      0\n",
       "4       California      1\n",
       "5         Colorado      0\n",
       "6      Connecticut      1\n",
       "7         Delaware      1\n",
       "8          Florida      1\n",
       "9          Georgia      1\n",
       "10          Hawaii      1\n",
       "11           Idaho      0\n",
       "12        Illinois      1\n",
       "13         Indiana      1\n",
       "14            Iowa      0\n",
       "15          Kansas      0\n",
       "16        Kentucky      0\n",
       "17       Louisiana      1\n",
       "18           Maine      1\n",
       "19        Maryland      1\n",
       "20   Massachusetts      1\n",
       "21        Michigan      1\n",
       "22       Minnesota      1\n",
       "23     Mississippi      1\n",
       "24        Missouri      0\n",
       "25         Montana      0\n",
       "26        Nebraska      0\n",
       "27          Nevada      0\n",
       "28   New Hampshire      1\n",
       "29      New Jersey      1\n",
       "30      New Mexico      0\n",
       "31        New York      1\n",
       "32  North Carolina      1\n",
       "33    North Dakota      0\n",
       "34            Ohio      1\n",
       "35        Oklahoma      0\n",
       "36          Oregon      1\n",
       "37    Pennsylvania      1\n",
       "38    Rhode Island      1\n",
       "39  South Carolina      1\n",
       "40    South Dakota      0\n",
       "41       Tennessee      0\n",
       "42           Texas      1\n",
       "43            Utah      0\n",
       "44         Vermont      0\n",
       "45        Virginia      1\n",
       "46      Washington      1\n",
       "47   West Virginia      0\n",
       "48       Wisconsin      1\n",
       "49         Wyoming      0"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coastline_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Table #5 (Cost of Living)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "      <th>Cost of Living Index</th>\n",
       "      <th>city_state</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Abilene</td>\n",
       "      <td>TX</td>\n",
       "      <td>89.1</td>\n",
       "      <td>Abilene, TX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Adrian</td>\n",
       "      <td>MI</td>\n",
       "      <td>90.5</td>\n",
       "      <td>Adrian, MI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Akron</td>\n",
       "      <td>OH</td>\n",
       "      <td>89.4</td>\n",
       "      <td>Akron, OH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Alamogordo</td>\n",
       "      <td>NM</td>\n",
       "      <td>85.8</td>\n",
       "      <td>Alamogordo, NM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Albany</td>\n",
       "      <td>GA</td>\n",
       "      <td>87.3</td>\n",
       "      <td>Albany, GA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>505</th>\n",
       "      <td>Wheeling</td>\n",
       "      <td>WV</td>\n",
       "      <td>84.1</td>\n",
       "      <td>Wheeling, WV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>506</th>\n",
       "      <td>New London</td>\n",
       "      <td>CT</td>\n",
       "      <td>105.9</td>\n",
       "      <td>New London, CT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>507</th>\n",
       "      <td>Daphne</td>\n",
       "      <td>AL</td>\n",
       "      <td>96.6</td>\n",
       "      <td>Daphne, AL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>508</th>\n",
       "      <td>Victoria</td>\n",
       "      <td>TX</td>\n",
       "      <td>89.5</td>\n",
       "      <td>Victoria, TX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>509</th>\n",
       "      <td>Aberdeen</td>\n",
       "      <td>WA</td>\n",
       "      <td>97.6</td>\n",
       "      <td>Aberdeen, WA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>510 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           City State  Cost of Living Index      city_state\n",
       "0       Abilene    TX                  89.1     Abilene, TX\n",
       "1        Adrian    MI                  90.5      Adrian, MI\n",
       "2         Akron    OH                  89.4       Akron, OH\n",
       "3    Alamogordo    NM                  85.8  Alamogordo, NM\n",
       "4        Albany    GA                  87.3      Albany, GA\n",
       "..          ...   ...                   ...             ...\n",
       "505    Wheeling    WV                  84.1    Wheeling, WV\n",
       "506  New London    CT                 105.9  New London, CT\n",
       "507      Daphne    AL                  96.6      Daphne, AL\n",
       "508    Victoria    TX                  89.5    Victoria, TX\n",
       "509    Aberdeen    WA                  97.6    Aberdeen, WA\n",
       "\n",
       "[510 rows x 4 columns]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('datasets/cost_of_living.csv')\n",
    "df[\"city_state\"] = df[\"City\"] + \", \" + df[\"State\"]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "geolocator = Nominatim(user_agent='find-county')\n",
    "\n",
    "# extract County from Geocode object \n",
    "def standardize_geocode(location):\n",
    "    try:\n",
    "        output = [x.strip() for x in location.address.split(',') if 'County' in x ][0]\n",
    "    except: \n",
    "        output = None\n",
    "        \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▍  | 381/510 [06:21<02:39,  1.23s/it]RateLimiter caught an error, retrying (0/2 tries). Called with (*('San Jose, CA',), **{}).\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/urllib3/connectionpool.py\", line 445, in _make_request\n",
      "    six.raise_from(e, None)\n",
      "  File \"<string>\", line 3, in raise_from\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/urllib3/connectionpool.py\", line 440, in _make_request\n",
      "    httplib_response = conn.getresponse()\n",
      "  File \"/opt/anaconda3/lib/python3.8/http/client.py\", line 1347, in getresponse\n",
      "    response.begin()\n",
      "  File \"/opt/anaconda3/lib/python3.8/http/client.py\", line 307, in begin\n",
      "    version, status, reason = self._read_status()\n",
      "  File \"/opt/anaconda3/lib/python3.8/http/client.py\", line 268, in _read_status\n",
      "    line = str(self.fp.readline(_MAXLINE + 1), \"iso-8859-1\")\n",
      "  File \"/opt/anaconda3/lib/python3.8/socket.py\", line 669, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "  File \"/opt/anaconda3/lib/python3.8/ssl.py\", line 1241, in recv_into\n",
      "    return self.read(nbytes, buffer)\n",
      "  File \"/opt/anaconda3/lib/python3.8/ssl.py\", line 1099, in read\n",
      "    return self._sslobj.read(len, buffer)\n",
      "socket.timeout: The read operation timed out\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/urllib3/connectionpool.py\", line 699, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/urllib3/connectionpool.py\", line 447, in _make_request\n",
      "    self._raise_timeout(err=e, url=url, timeout_value=read_timeout)\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/urllib3/connectionpool.py\", line 336, in _raise_timeout\n",
      "    raise ReadTimeoutError(\n",
      "urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Read timed out. (read timeout=1)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/requests/adapters.py\", line 439, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/urllib3/connectionpool.py\", line 783, in urlopen\n",
      "    return self.urlopen(\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/urllib3/connectionpool.py\", line 783, in urlopen\n",
      "    return self.urlopen(\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/urllib3/connectionpool.py\", line 755, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/urllib3/util/retry.py\", line 574, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Max retries exceeded with url: /search?q=San+Jose%2C+CA&format=json&limit=1 (Caused by ReadTimeoutError(\"HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Read timed out. (read timeout=1)\"))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/geopy/adapters.py\", line 448, in _request\n",
      "    resp = self.session.get(url, timeout=timeout, headers=headers)\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/requests/sessions.py\", line 555, in get\n",
      "    return self.request('GET', url, **kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/requests/sessions.py\", line 542, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/requests/sessions.py\", line 655, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/requests/adapters.py\", line 516, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Max retries exceeded with url: /search?q=San+Jose%2C+CA&format=json&limit=1 (Caused by ReadTimeoutError(\"HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Read timed out. (read timeout=1)\"))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/geopy/extra/rate_limiter.py\", line 136, in _retries_gen\n",
      "    yield i  # Run the function.\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/geopy/extra/rate_limiter.py\", line 274, in __call__\n",
      "    res = self.func(*args, **kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/geopy/geocoders/nominatim.py\", line 297, in geocode\n",
      "    return self._call_geocoder(url, callback, timeout=timeout)\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/geopy/geocoders/base.py\", line 368, in _call_geocoder\n",
      "    result = self.adapter.get_json(url, timeout=timeout, headers=req_headers)\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/geopy/adapters.py\", line 438, in get_json\n",
      "    resp = self._request(url, timeout=timeout, headers=headers)\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/geopy/adapters.py\", line 460, in _request\n",
      "    raise GeocoderUnavailable(message)\n",
      "geopy.exc.GeocoderUnavailable: HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Max retries exceeded with url: /search?q=San+Jose%2C+CA&format=json&limit=1 (Caused by ReadTimeoutError(\"HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Read timed out. (read timeout=1)\"))\n",
      "RateLimiter caught an error, retrying (1/2 tries). Called with (*('San Jose, CA',), **{}).\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/urllib3/connectionpool.py\", line 445, in _make_request\n",
      "    six.raise_from(e, None)\n",
      "  File \"<string>\", line 3, in raise_from\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/urllib3/connectionpool.py\", line 440, in _make_request\n",
      "    httplib_response = conn.getresponse()\n",
      "  File \"/opt/anaconda3/lib/python3.8/http/client.py\", line 1347, in getresponse\n",
      "    response.begin()\n",
      "  File \"/opt/anaconda3/lib/python3.8/http/client.py\", line 307, in begin\n",
      "    version, status, reason = self._read_status()\n",
      "  File \"/opt/anaconda3/lib/python3.8/http/client.py\", line 268, in _read_status\n",
      "    line = str(self.fp.readline(_MAXLINE + 1), \"iso-8859-1\")\n",
      "  File \"/opt/anaconda3/lib/python3.8/socket.py\", line 669, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "  File \"/opt/anaconda3/lib/python3.8/ssl.py\", line 1241, in recv_into\n",
      "    return self.read(nbytes, buffer)\n",
      "  File \"/opt/anaconda3/lib/python3.8/ssl.py\", line 1099, in read\n",
      "    return self._sslobj.read(len, buffer)\n",
      "socket.timeout: The read operation timed out\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/urllib3/connectionpool.py\", line 699, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/urllib3/connectionpool.py\", line 447, in _make_request\n",
      "    self._raise_timeout(err=e, url=url, timeout_value=read_timeout)\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/urllib3/connectionpool.py\", line 336, in _raise_timeout\n",
      "    raise ReadTimeoutError(\n",
      "urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Read timed out. (read timeout=1)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/requests/adapters.py\", line 439, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/urllib3/connectionpool.py\", line 783, in urlopen\n",
      "    return self.urlopen(\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/urllib3/connectionpool.py\", line 783, in urlopen\n",
      "    return self.urlopen(\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/urllib3/connectionpool.py\", line 755, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/urllib3/util/retry.py\", line 574, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Max retries exceeded with url: /search?q=San+Jose%2C+CA&format=json&limit=1 (Caused by ReadTimeoutError(\"HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Read timed out. (read timeout=1)\"))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/geopy/adapters.py\", line 448, in _request\n",
      "    resp = self.session.get(url, timeout=timeout, headers=headers)\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/requests/sessions.py\", line 555, in get\n",
      "    return self.request('GET', url, **kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/requests/sessions.py\", line 542, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/requests/sessions.py\", line 655, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/requests/adapters.py\", line 516, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Max retries exceeded with url: /search?q=San+Jose%2C+CA&format=json&limit=1 (Caused by ReadTimeoutError(\"HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Read timed out. (read timeout=1)\"))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/geopy/extra/rate_limiter.py\", line 136, in _retries_gen\n",
      "    yield i  # Run the function.\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/geopy/extra/rate_limiter.py\", line 274, in __call__\n",
      "    res = self.func(*args, **kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/geopy/geocoders/nominatim.py\", line 297, in geocode\n",
      "    return self._call_geocoder(url, callback, timeout=timeout)\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/geopy/geocoders/base.py\", line 368, in _call_geocoder\n",
      "    result = self.adapter.get_json(url, timeout=timeout, headers=req_headers)\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/geopy/adapters.py\", line 438, in get_json\n",
      "    resp = self._request(url, timeout=timeout, headers=headers)\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/geopy/adapters.py\", line 460, in _request\n",
      "    raise GeocoderUnavailable(message)\n",
      "geopy.exc.GeocoderUnavailable: HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Max retries exceeded with url: /search?q=San+Jose%2C+CA&format=json&limit=1 (Caused by ReadTimeoutError(\"HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Read timed out. (read timeout=1)\"))\n",
      "RateLimiter swallowed an error after 2 retries. Called with (*('San Jose, CA',), **{}).\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/urllib3/connectionpool.py\", line 445, in _make_request\n",
      "    six.raise_from(e, None)\n",
      "  File \"<string>\", line 3, in raise_from\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/urllib3/connectionpool.py\", line 440, in _make_request\n",
      "    httplib_response = conn.getresponse()\n",
      "  File \"/opt/anaconda3/lib/python3.8/http/client.py\", line 1347, in getresponse\n",
      "    response.begin()\n",
      "  File \"/opt/anaconda3/lib/python3.8/http/client.py\", line 307, in begin\n",
      "    version, status, reason = self._read_status()\n",
      "  File \"/opt/anaconda3/lib/python3.8/http/client.py\", line 268, in _read_status\n",
      "    line = str(self.fp.readline(_MAXLINE + 1), \"iso-8859-1\")\n",
      "  File \"/opt/anaconda3/lib/python3.8/socket.py\", line 669, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "  File \"/opt/anaconda3/lib/python3.8/ssl.py\", line 1241, in recv_into\n",
      "    return self.read(nbytes, buffer)\n",
      "  File \"/opt/anaconda3/lib/python3.8/ssl.py\", line 1099, in read\n",
      "    return self._sslobj.read(len, buffer)\n",
      "socket.timeout: The read operation timed out\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/urllib3/connectionpool.py\", line 699, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/urllib3/connectionpool.py\", line 447, in _make_request\n",
      "    self._raise_timeout(err=e, url=url, timeout_value=read_timeout)\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/urllib3/connectionpool.py\", line 336, in _raise_timeout\n",
      "    raise ReadTimeoutError(\n",
      "urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Read timed out. (read timeout=1)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/requests/adapters.py\", line 439, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/urllib3/connectionpool.py\", line 783, in urlopen\n",
      "    return self.urlopen(\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/urllib3/connectionpool.py\", line 783, in urlopen\n",
      "    return self.urlopen(\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/urllib3/connectionpool.py\", line 755, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/urllib3/util/retry.py\", line 574, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Max retries exceeded with url: /search?q=San+Jose%2C+CA&format=json&limit=1 (Caused by ReadTimeoutError(\"HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Read timed out. (read timeout=1)\"))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/geopy/adapters.py\", line 448, in _request\n",
      "    resp = self.session.get(url, timeout=timeout, headers=headers)\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/requests/sessions.py\", line 555, in get\n",
      "    return self.request('GET', url, **kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/requests/sessions.py\", line 542, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/requests/sessions.py\", line 655, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/requests/adapters.py\", line 516, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Max retries exceeded with url: /search?q=San+Jose%2C+CA&format=json&limit=1 (Caused by ReadTimeoutError(\"HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Read timed out. (read timeout=1)\"))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/geopy/extra/rate_limiter.py\", line 274, in __call__\n",
      "    res = self.func(*args, **kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/geopy/geocoders/nominatim.py\", line 297, in geocode\n",
      "    return self._call_geocoder(url, callback, timeout=timeout)\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/geopy/geocoders/base.py\", line 368, in _call_geocoder\n",
      "    result = self.adapter.get_json(url, timeout=timeout, headers=req_headers)\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/geopy/adapters.py\", line 438, in get_json\n",
      "    resp = self._request(url, timeout=timeout, headers=headers)\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/geopy/adapters.py\", line 460, in _request\n",
      "    raise GeocoderUnavailable(message)\n",
      "geopy.exc.GeocoderUnavailable: HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Max retries exceeded with url: /search?q=San+Jose%2C+CA&format=json&limit=1 (Caused by ReadTimeoutError(\"HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Read timed out. (read timeout=1)\"))\n",
      " 96%|█████████▌| 489/510 [08:28<00:20,  1.00it/s]RateLimiter caught an error, retrying (0/2 tries). Called with (*('Santa Maria, CA',), **{}).\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/urllib3/connectionpool.py\", line 445, in _make_request\n",
      "    six.raise_from(e, None)\n",
      "  File \"<string>\", line 3, in raise_from\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/urllib3/connectionpool.py\", line 440, in _make_request\n",
      "    httplib_response = conn.getresponse()\n",
      "  File \"/opt/anaconda3/lib/python3.8/http/client.py\", line 1347, in getresponse\n",
      "    response.begin()\n",
      "  File \"/opt/anaconda3/lib/python3.8/http/client.py\", line 307, in begin\n",
      "    version, status, reason = self._read_status()\n",
      "  File \"/opt/anaconda3/lib/python3.8/http/client.py\", line 268, in _read_status\n",
      "    line = str(self.fp.readline(_MAXLINE + 1), \"iso-8859-1\")\n",
      "  File \"/opt/anaconda3/lib/python3.8/socket.py\", line 669, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "  File \"/opt/anaconda3/lib/python3.8/ssl.py\", line 1241, in recv_into\n",
      "    return self.read(nbytes, buffer)\n",
      "  File \"/opt/anaconda3/lib/python3.8/ssl.py\", line 1099, in read\n",
      "    return self._sslobj.read(len, buffer)\n",
      "socket.timeout: The read operation timed out\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/urllib3/connectionpool.py\", line 699, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/urllib3/connectionpool.py\", line 447, in _make_request\n",
      "    self._raise_timeout(err=e, url=url, timeout_value=read_timeout)\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/urllib3/connectionpool.py\", line 336, in _raise_timeout\n",
      "    raise ReadTimeoutError(\n",
      "urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Read timed out. (read timeout=1)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/requests/adapters.py\", line 439, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/urllib3/connectionpool.py\", line 783, in urlopen\n",
      "    return self.urlopen(\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/urllib3/connectionpool.py\", line 783, in urlopen\n",
      "    return self.urlopen(\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/urllib3/connectionpool.py\", line 755, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/urllib3/util/retry.py\", line 574, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Max retries exceeded with url: /search?q=Santa+Maria%2C+CA&format=json&limit=1 (Caused by ReadTimeoutError(\"HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Read timed out. (read timeout=1)\"))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/geopy/adapters.py\", line 448, in _request\n",
      "    resp = self.session.get(url, timeout=timeout, headers=headers)\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/requests/sessions.py\", line 555, in get\n",
      "    return self.request('GET', url, **kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/requests/sessions.py\", line 542, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/requests/sessions.py\", line 655, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/requests/adapters.py\", line 516, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Max retries exceeded with url: /search?q=Santa+Maria%2C+CA&format=json&limit=1 (Caused by ReadTimeoutError(\"HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Read timed out. (read timeout=1)\"))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/geopy/extra/rate_limiter.py\", line 136, in _retries_gen\n",
      "    yield i  # Run the function.\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/geopy/extra/rate_limiter.py\", line 274, in __call__\n",
      "    res = self.func(*args, **kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/geopy/geocoders/nominatim.py\", line 297, in geocode\n",
      "    return self._call_geocoder(url, callback, timeout=timeout)\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/geopy/geocoders/base.py\", line 368, in _call_geocoder\n",
      "    result = self.adapter.get_json(url, timeout=timeout, headers=req_headers)\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/geopy/adapters.py\", line 438, in get_json\n",
      "    resp = self._request(url, timeout=timeout, headers=headers)\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/geopy/adapters.py\", line 460, in _request\n",
      "    raise GeocoderUnavailable(message)\n",
      "geopy.exc.GeocoderUnavailable: HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Max retries exceeded with url: /search?q=Santa+Maria%2C+CA&format=json&limit=1 (Caused by ReadTimeoutError(\"HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Read timed out. (read timeout=1)\"))\n",
      "RateLimiter caught an error, retrying (1/2 tries). Called with (*('Santa Maria, CA',), **{}).\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/urllib3/connectionpool.py\", line 445, in _make_request\n",
      "    six.raise_from(e, None)\n",
      "  File \"<string>\", line 3, in raise_from\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/urllib3/connectionpool.py\", line 440, in _make_request\n",
      "    httplib_response = conn.getresponse()\n",
      "  File \"/opt/anaconda3/lib/python3.8/http/client.py\", line 1347, in getresponse\n",
      "    response.begin()\n",
      "  File \"/opt/anaconda3/lib/python3.8/http/client.py\", line 307, in begin\n",
      "    version, status, reason = self._read_status()\n",
      "  File \"/opt/anaconda3/lib/python3.8/http/client.py\", line 268, in _read_status\n",
      "    line = str(self.fp.readline(_MAXLINE + 1), \"iso-8859-1\")\n",
      "  File \"/opt/anaconda3/lib/python3.8/socket.py\", line 669, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "  File \"/opt/anaconda3/lib/python3.8/ssl.py\", line 1241, in recv_into\n",
      "    return self.read(nbytes, buffer)\n",
      "  File \"/opt/anaconda3/lib/python3.8/ssl.py\", line 1099, in read\n",
      "    return self._sslobj.read(len, buffer)\n",
      "socket.timeout: The read operation timed out\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/urllib3/connectionpool.py\", line 699, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/urllib3/connectionpool.py\", line 447, in _make_request\n",
      "    self._raise_timeout(err=e, url=url, timeout_value=read_timeout)\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/urllib3/connectionpool.py\", line 336, in _raise_timeout\n",
      "    raise ReadTimeoutError(\n",
      "urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Read timed out. (read timeout=1)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/requests/adapters.py\", line 439, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/urllib3/connectionpool.py\", line 783, in urlopen\n",
      "    return self.urlopen(\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/urllib3/connectionpool.py\", line 783, in urlopen\n",
      "    return self.urlopen(\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/urllib3/connectionpool.py\", line 755, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/urllib3/util/retry.py\", line 574, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Max retries exceeded with url: /search?q=Santa+Maria%2C+CA&format=json&limit=1 (Caused by ReadTimeoutError(\"HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Read timed out. (read timeout=1)\"))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/geopy/adapters.py\", line 448, in _request\n",
      "    resp = self.session.get(url, timeout=timeout, headers=headers)\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/requests/sessions.py\", line 555, in get\n",
      "    return self.request('GET', url, **kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/requests/sessions.py\", line 542, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/requests/sessions.py\", line 655, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/requests/adapters.py\", line 516, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Max retries exceeded with url: /search?q=Santa+Maria%2C+CA&format=json&limit=1 (Caused by ReadTimeoutError(\"HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Read timed out. (read timeout=1)\"))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/geopy/extra/rate_limiter.py\", line 136, in _retries_gen\n",
      "    yield i  # Run the function.\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/geopy/extra/rate_limiter.py\", line 274, in __call__\n",
      "    res = self.func(*args, **kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/geopy/geocoders/nominatim.py\", line 297, in geocode\n",
      "    return self._call_geocoder(url, callback, timeout=timeout)\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/geopy/geocoders/base.py\", line 368, in _call_geocoder\n",
      "    result = self.adapter.get_json(url, timeout=timeout, headers=req_headers)\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/geopy/adapters.py\", line 438, in get_json\n",
      "    resp = self._request(url, timeout=timeout, headers=headers)\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/geopy/adapters.py\", line 460, in _request\n",
      "    raise GeocoderUnavailable(message)\n",
      "geopy.exc.GeocoderUnavailable: HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Max retries exceeded with url: /search?q=Santa+Maria%2C+CA&format=json&limit=1 (Caused by ReadTimeoutError(\"HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Read timed out. (read timeout=1)\"))\n",
      "RateLimiter swallowed an error after 2 retries. Called with (*('Santa Maria, CA',), **{}).\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/urllib3/connectionpool.py\", line 445, in _make_request\n",
      "    six.raise_from(e, None)\n",
      "  File \"<string>\", line 3, in raise_from\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/urllib3/connectionpool.py\", line 440, in _make_request\n",
      "    httplib_response = conn.getresponse()\n",
      "  File \"/opt/anaconda3/lib/python3.8/http/client.py\", line 1347, in getresponse\n",
      "    response.begin()\n",
      "  File \"/opt/anaconda3/lib/python3.8/http/client.py\", line 307, in begin\n",
      "    version, status, reason = self._read_status()\n",
      "  File \"/opt/anaconda3/lib/python3.8/http/client.py\", line 268, in _read_status\n",
      "    line = str(self.fp.readline(_MAXLINE + 1), \"iso-8859-1\")\n",
      "  File \"/opt/anaconda3/lib/python3.8/socket.py\", line 669, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "  File \"/opt/anaconda3/lib/python3.8/ssl.py\", line 1241, in recv_into\n",
      "    return self.read(nbytes, buffer)\n",
      "  File \"/opt/anaconda3/lib/python3.8/ssl.py\", line 1099, in read\n",
      "    return self._sslobj.read(len, buffer)\n",
      "socket.timeout: The read operation timed out\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/urllib3/connectionpool.py\", line 699, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/urllib3/connectionpool.py\", line 447, in _make_request\n",
      "    self._raise_timeout(err=e, url=url, timeout_value=read_timeout)\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/urllib3/connectionpool.py\", line 336, in _raise_timeout\n",
      "    raise ReadTimeoutError(\n",
      "urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Read timed out. (read timeout=1)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/requests/adapters.py\", line 439, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/urllib3/connectionpool.py\", line 783, in urlopen\n",
      "    return self.urlopen(\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/urllib3/connectionpool.py\", line 783, in urlopen\n",
      "    return self.urlopen(\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/urllib3/connectionpool.py\", line 755, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/urllib3/util/retry.py\", line 574, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Max retries exceeded with url: /search?q=Santa+Maria%2C+CA&format=json&limit=1 (Caused by ReadTimeoutError(\"HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Read timed out. (read timeout=1)\"))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/geopy/adapters.py\", line 448, in _request\n",
      "    resp = self.session.get(url, timeout=timeout, headers=headers)\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/requests/sessions.py\", line 555, in get\n",
      "    return self.request('GET', url, **kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/requests/sessions.py\", line 542, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/requests/sessions.py\", line 655, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/requests/adapters.py\", line 516, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Max retries exceeded with url: /search?q=Santa+Maria%2C+CA&format=json&limit=1 (Caused by ReadTimeoutError(\"HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Read timed out. (read timeout=1)\"))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/geopy/extra/rate_limiter.py\", line 274, in __call__\n",
      "    res = self.func(*args, **kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/geopy/geocoders/nominatim.py\", line 297, in geocode\n",
      "    return self._call_geocoder(url, callback, timeout=timeout)\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/geopy/geocoders/base.py\", line 368, in _call_geocoder\n",
      "    result = self.adapter.get_json(url, timeout=timeout, headers=req_headers)\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/geopy/adapters.py\", line 438, in get_json\n",
      "    resp = self._request(url, timeout=timeout, headers=headers)\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/geopy/adapters.py\", line 460, in _request\n",
      "    raise GeocoderUnavailable(message)\n",
      "geopy.exc.GeocoderUnavailable: HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Max retries exceeded with url: /search?q=Santa+Maria%2C+CA&format=json&limit=1 (Caused by ReadTimeoutError(\"HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Read timed out. (read timeout=1)\"))\n",
      "100%|██████████| 510/510 [09:10<00:00,  1.08s/it]\n"
     ]
    }
   ],
   "source": [
    "# we need this to avoid timeout \n",
    "geocode = RateLimiter(geolocator.geocode, min_delay_seconds=1)\n",
    "\n",
    "# allow for progress bar on .apply()\n",
    "tqdm.pandas()\n",
    "\n",
    "# convert city_state column to geocode\n",
    "try:\n",
    "    df['geocode'] = df['city_state'].progress_apply(geocode)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert geocode column to extract the county\n",
    "df['county'] = df['geocode'].apply(standardize_geocode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check for nans\n",
    "df.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "      <th>Cost of Living Index</th>\n",
       "      <th>city_state</th>\n",
       "      <th>geocode</th>\n",
       "      <th>county</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Abilene</td>\n",
       "      <td>TX</td>\n",
       "      <td>89.1</td>\n",
       "      <td>Abilene, TX</td>\n",
       "      <td>(Abilene, Taylor County, Texas, 79697, United ...</td>\n",
       "      <td>Taylor County</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Adrian</td>\n",
       "      <td>MI</td>\n",
       "      <td>90.5</td>\n",
       "      <td>Adrian, MI</td>\n",
       "      <td>(Adrian, Lenawee County, Michigan, 49221, Unit...</td>\n",
       "      <td>Lenawee County</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Akron</td>\n",
       "      <td>OH</td>\n",
       "      <td>89.4</td>\n",
       "      <td>Akron, OH</td>\n",
       "      <td>(Akron, Summit County, Ohio, United States, (4...</td>\n",
       "      <td>Summit County</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Alamogordo</td>\n",
       "      <td>NM</td>\n",
       "      <td>85.8</td>\n",
       "      <td>Alamogordo, NM</td>\n",
       "      <td>(Alamogordo, Otero County, New Mexico, 88310, ...</td>\n",
       "      <td>Otero County</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Albany</td>\n",
       "      <td>GA</td>\n",
       "      <td>87.3</td>\n",
       "      <td>Albany, GA</td>\n",
       "      <td>(Albany, Dougherty County, Georgia, 31701, Uni...</td>\n",
       "      <td>Dougherty County</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>505</th>\n",
       "      <td>Wheeling</td>\n",
       "      <td>WV</td>\n",
       "      <td>84.1</td>\n",
       "      <td>Wheeling, WV</td>\n",
       "      <td>(Wheeling, Ohio County, West Virginia, 26003, ...</td>\n",
       "      <td>Ohio County</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>506</th>\n",
       "      <td>New London</td>\n",
       "      <td>CT</td>\n",
       "      <td>105.9</td>\n",
       "      <td>New London, CT</td>\n",
       "      <td>(New London, New London County, Connecticut, 0...</td>\n",
       "      <td>New London County</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>507</th>\n",
       "      <td>Daphne</td>\n",
       "      <td>AL</td>\n",
       "      <td>96.6</td>\n",
       "      <td>Daphne, AL</td>\n",
       "      <td>(Daphne, Baldwin County, Alabama, 35626, Unite...</td>\n",
       "      <td>Baldwin County</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>508</th>\n",
       "      <td>Victoria</td>\n",
       "      <td>TX</td>\n",
       "      <td>89.5</td>\n",
       "      <td>Victoria, TX</td>\n",
       "      <td>(Victoria County, Texas, United States, (28.80...</td>\n",
       "      <td>Victoria County</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>509</th>\n",
       "      <td>Aberdeen</td>\n",
       "      <td>WA</td>\n",
       "      <td>97.6</td>\n",
       "      <td>Aberdeen, WA</td>\n",
       "      <td>(Aberdeen, Grays Harbor County, Washington, 98...</td>\n",
       "      <td>Grays Harbor County</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>478 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           City State  Cost of Living Index      city_state  \\\n",
       "0       Abilene    TX                  89.1     Abilene, TX   \n",
       "1        Adrian    MI                  90.5      Adrian, MI   \n",
       "2         Akron    OH                  89.4       Akron, OH   \n",
       "3    Alamogordo    NM                  85.8  Alamogordo, NM   \n",
       "4        Albany    GA                  87.3      Albany, GA   \n",
       "..          ...   ...                   ...             ...   \n",
       "505    Wheeling    WV                  84.1    Wheeling, WV   \n",
       "506  New London    CT                 105.9  New London, CT   \n",
       "507      Daphne    AL                  96.6      Daphne, AL   \n",
       "508    Victoria    TX                  89.5    Victoria, TX   \n",
       "509    Aberdeen    WA                  97.6    Aberdeen, WA   \n",
       "\n",
       "                                               geocode               county  \n",
       "0    (Abilene, Taylor County, Texas, 79697, United ...        Taylor County  \n",
       "1    (Adrian, Lenawee County, Michigan, 49221, Unit...       Lenawee County  \n",
       "2    (Akron, Summit County, Ohio, United States, (4...        Summit County  \n",
       "3    (Alamogordo, Otero County, New Mexico, 88310, ...         Otero County  \n",
       "4    (Albany, Dougherty County, Georgia, 31701, Uni...     Dougherty County  \n",
       "..                                                 ...                  ...  \n",
       "505  (Wheeling, Ohio County, West Virginia, 26003, ...          Ohio County  \n",
       "506  (New London, New London County, Connecticut, 0...    New London County  \n",
       "507  (Daphne, Baldwin County, Alabama, 35626, Unite...       Baldwin County  \n",
       "508  (Victoria County, Texas, United States, (28.80...      Victoria County  \n",
       "509  (Aberdeen, Grays Harbor County, Washington, 98...  Grays Harbor County  \n",
       "\n",
       "[478 rows x 6 columns]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove rows with nans\n",
    "df.dropna(how='any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "      <th>Cost of Living Index</th>\n",
       "      <th>city_state</th>\n",
       "      <th>geocode</th>\n",
       "      <th>county</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Abilene</td>\n",
       "      <td>TX</td>\n",
       "      <td>89.1</td>\n",
       "      <td>Abilene, TX</td>\n",
       "      <td>(Abilene, Taylor County, Texas, 79697, United ...</td>\n",
       "      <td>Taylor County</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Adrian</td>\n",
       "      <td>MI</td>\n",
       "      <td>90.5</td>\n",
       "      <td>Adrian, MI</td>\n",
       "      <td>(Adrian, Lenawee County, Michigan, 49221, Unit...</td>\n",
       "      <td>Lenawee County</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Akron</td>\n",
       "      <td>OH</td>\n",
       "      <td>89.4</td>\n",
       "      <td>Akron, OH</td>\n",
       "      <td>(Akron, Summit County, Ohio, United States, (4...</td>\n",
       "      <td>Summit County</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Alamogordo</td>\n",
       "      <td>NM</td>\n",
       "      <td>85.8</td>\n",
       "      <td>Alamogordo, NM</td>\n",
       "      <td>(Alamogordo, Otero County, New Mexico, 88310, ...</td>\n",
       "      <td>Otero County</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Albany</td>\n",
       "      <td>GA</td>\n",
       "      <td>87.3</td>\n",
       "      <td>Albany, GA</td>\n",
       "      <td>(Albany, Dougherty County, Georgia, 31701, Uni...</td>\n",
       "      <td>Dougherty County</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>505</th>\n",
       "      <td>Wheeling</td>\n",
       "      <td>WV</td>\n",
       "      <td>84.1</td>\n",
       "      <td>Wheeling, WV</td>\n",
       "      <td>(Wheeling, Ohio County, West Virginia, 26003, ...</td>\n",
       "      <td>Ohio County</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>506</th>\n",
       "      <td>New London</td>\n",
       "      <td>CT</td>\n",
       "      <td>105.9</td>\n",
       "      <td>New London, CT</td>\n",
       "      <td>(New London, New London County, Connecticut, 0...</td>\n",
       "      <td>New London County</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>507</th>\n",
       "      <td>Daphne</td>\n",
       "      <td>AL</td>\n",
       "      <td>96.6</td>\n",
       "      <td>Daphne, AL</td>\n",
       "      <td>(Daphne, Baldwin County, Alabama, 35626, Unite...</td>\n",
       "      <td>Baldwin County</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>508</th>\n",
       "      <td>Victoria</td>\n",
       "      <td>TX</td>\n",
       "      <td>89.5</td>\n",
       "      <td>Victoria, TX</td>\n",
       "      <td>(Victoria County, Texas, United States, (28.80...</td>\n",
       "      <td>Victoria County</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>509</th>\n",
       "      <td>Aberdeen</td>\n",
       "      <td>WA</td>\n",
       "      <td>97.6</td>\n",
       "      <td>Aberdeen, WA</td>\n",
       "      <td>(Aberdeen, Grays Harbor County, Washington, 98...</td>\n",
       "      <td>Grays Harbor County</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>510 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           City State  Cost of Living Index      city_state  \\\n",
       "0       Abilene    TX                  89.1     Abilene, TX   \n",
       "1        Adrian    MI                  90.5      Adrian, MI   \n",
       "2         Akron    OH                  89.4       Akron, OH   \n",
       "3    Alamogordo    NM                  85.8  Alamogordo, NM   \n",
       "4        Albany    GA                  87.3      Albany, GA   \n",
       "..          ...   ...                   ...             ...   \n",
       "505    Wheeling    WV                  84.1    Wheeling, WV   \n",
       "506  New London    CT                 105.9  New London, CT   \n",
       "507      Daphne    AL                  96.6      Daphne, AL   \n",
       "508    Victoria    TX                  89.5    Victoria, TX   \n",
       "509    Aberdeen    WA                  97.6    Aberdeen, WA   \n",
       "\n",
       "                                               geocode               county  \n",
       "0    (Abilene, Taylor County, Texas, 79697, United ...        Taylor County  \n",
       "1    (Adrian, Lenawee County, Michigan, 49221, Unit...       Lenawee County  \n",
       "2    (Akron, Summit County, Ohio, United States, (4...        Summit County  \n",
       "3    (Alamogordo, Otero County, New Mexico, 88310, ...         Otero County  \n",
       "4    (Albany, Dougherty County, Georgia, 31701, Uni...     Dougherty County  \n",
       "..                                                 ...                  ...  \n",
       "505  (Wheeling, Ohio County, West Virginia, 26003, ...          Ohio County  \n",
       "506  (New London, New London County, Connecticut, 0...    New London County  \n",
       "507  (Daphne, Baldwin County, Alabama, 35626, Unite...       Baldwin County  \n",
       "508  (Victoria County, Texas, United States, (28.80...      Victoria County  \n",
       "509  (Aberdeen, Grays Harbor County, Washington, 98...  Grays Harbor County  \n",
       "\n",
       "[510 rows x 6 columns]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # save resulting df as csv to avoid running previous cells frequently\n",
    "# df.to_csv('datasets/col_updated.csv', index=False)\n",
    "df.to_csv('datasets/cleaned/col_updated.csv', index=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Table #6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>State</th>\n",
       "      <th>County</th>\n",
       "      <th>Temperature (F)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AL</td>\n",
       "      <td>Autauga County</td>\n",
       "      <td>65.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AL</td>\n",
       "      <td>Baldwin County</td>\n",
       "      <td>68.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AL</td>\n",
       "      <td>Barbour County</td>\n",
       "      <td>66.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AL</td>\n",
       "      <td>Bibb County</td>\n",
       "      <td>63.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AL</td>\n",
       "      <td>Blount County</td>\n",
       "      <td>62.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3132</th>\n",
       "      <td>AK</td>\n",
       "      <td>Skagway Municipality</td>\n",
       "      <td>30.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3133</th>\n",
       "      <td>AK</td>\n",
       "      <td>Southeast Fairbanks Census Area</td>\n",
       "      <td>25.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3134</th>\n",
       "      <td>AK</td>\n",
       "      <td>Wrangell City and Borough</td>\n",
       "      <td>40.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3135</th>\n",
       "      <td>AK</td>\n",
       "      <td>Yakutat City and Borough</td>\n",
       "      <td>33.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3136</th>\n",
       "      <td>AK</td>\n",
       "      <td>Yukon-Koyukuk Census Area</td>\n",
       "      <td>27.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3137 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     State                           County  Temperature (F)\n",
       "0       AL                   Autauga County             65.7\n",
       "1       AL                   Baldwin County             68.4\n",
       "2       AL                   Barbour County             66.0\n",
       "3       AL                      Bibb County             63.9\n",
       "4       AL                    Blount County             62.7\n",
       "...    ...                              ...              ...\n",
       "3132    AK             Skagway Municipality             30.9\n",
       "3133    AK  Southeast Fairbanks Census Area             25.7\n",
       "3134    AK        Wrangell City and Borough             40.0\n",
       "3135    AK         Yakutat City and Borough             33.9\n",
       "3136    AK        Yukon-Koyukuk Census Area             27.2\n",
       "\n",
       "[3137 rows x 3 columns]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dataset for the mean climate for each county in the USA except those in Hawaii. The climate is measured over a 5-year span from 2017-2022. \n",
    "climate_df = pd.read_csv('datasets/Average_Climate_By_County.csv')\n",
    "climate_df['Location ID'] = climate_df['Location ID'].apply([lambda x: x[0:2]])\n",
    "climate_df = climate_df.rename(columns={'Location ID': 'State', 'Location': 'County', 'Value': 'Temperature (F)'})\n",
    "climate_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
