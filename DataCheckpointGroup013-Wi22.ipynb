{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COGS 108 - Data Checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Names\n",
    "\n",
    "- Tyler Le\n",
    "- Aditya Tomar\n",
    "- William Lynch\n",
    "- Michael Mao\n",
    "- Natalie Quach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='research_question'></a>\n",
    "# Research Question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is there a positive correlation between the cost of living and the impact of natural disasters in terms of injuries, casualties, and property damage per capita at the county level? Furthermore, in which state does the impact of natural disasters affect cost of living the most?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset(s)\n",
    "Please note that all data file names correspond to their name in our \"datasets\" folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Frequency of Disasters By State**\n",
    "- Dataset Name: Billion-Dollar Disasters By Year (CPI-Adjusted) (natural_disaster_frequencies.csv)\n",
    "- Link to the dataset: https://www.ncdc.noaa.gov/billions/state-freq-data.csv\n",
    "- Number of observations: 2228 \n",
    "- Number of features: 9. These features are mostly numerical.\n",
    "\n",
    "This dataset contains the count of natural disasters that cost more than 1 billion dollars for each state from 1980 - 2021. Each observation contains one state and the count of each natural disaster type for a given year.\n",
    "\n",
    "**2. Types of Disaster By State/County**\n",
    "- Dataset Name: Disaster Declarations Summaries (DisasterDeclarationsSummaries.csv)\n",
    "- Link to the dataset: https://www.fema.gov/api/open/v1/DisasterDeclarationsSummaries.csv\n",
    "- Number of observations: 62771\n",
    "- Number of features: 23. Contains a combination of numerical and string data types.\n",
    "\n",
    "This dataset contains all federally declared natural disasters from 1953-2022 (by year, state, county, and type) along with declared recovery programs. Each observation contains the type of natural disaster, when it occurred, and the state/county it occured in.\n",
    "\n",
    "\n",
    "**3. National Risk Index (NRI)**\n",
    "- Dataset Name: National Risk Index per County (NRI_Table_Counties.csv)\n",
    "- Link to the dataset: https://hazards.fema.gov/nri/data-resources#csvDownload\n",
    "- Number of observations: 3142\n",
    "- Number of features: 365. Contains a combination of numerical and string data types.\n",
    "\n",
    "Dataset from FEMA that identifies counties and states most at risk to 18 natural hazards. Includes data about expected annual losses from natural hazards, social vulnerability and community resilience. \n",
    "\n",
    "**4. States with Coastline**\n",
    "- Dataset Name: States with Coastline (states_with_coastline.csv)\n",
    "- Link to the dataset: https://worldpopulationreview.com/state-rankings/coastal-states\n",
    "- Number of observations: 50\n",
    "- Number of features: 2. Contains string data types.\n",
    "\n",
    "This dataset contains whether or not each state in the United States has a coast. Each observation contains a state and its associated coast.\n",
    "\n",
    "\n",
    "**5. Cost of Living**\n",
    "- Dataset Name: Cost of Living (cost_of_living.csv)\n",
    "- Link to the dataset: https://advisorsmith.com/wp-content/uploads/2021/02/advisorsmith_cost_of_living_index.csv\n",
    "- Number of observations: 510\n",
    "- Number of features: 3. Contains numerical and string data types.\n",
    "\n",
    "Each observation in this dataset contains a state, the city associated with the state, and the Cost of Living Index. The Cost of Living Index measures the costs such as food and energy.\n",
    "\n",
    "**6. Climate**\n",
    "- Average Climate by County (Average_Climate_By_County.csv)\n",
    "- https://www.ncdc.noaa.gov/cag/county/mapping\n",
    "- Number of observations: 3137\n",
    "- Number of features: 3. Contains a combination of numerical and string types.\n",
    "\n",
    "This dataset contains the mean climate over a 5-year span from 2017 to 2022 for all counties in the USA except those in Hawaii. The climate is measured in Fahrenheit. This dataset is for comparing the correlation between natural disasters vs cost of living with climate vs cost of living because climate is a potential confounding variable that affects cost of living.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from geopy.geocoders import Nominatim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "**Table #1 (Frequency of Disaster By State):** This dataset was fairly clean in that there were no missing values and each observation contains whether or not a state had a natural disaster event in a certain year. We decided to remove spaces from the column names and to replace them with underscores. Since each column describing a natural disaster contains the count of how many natural disasters of that type, there was little data cleaning needed for this dataset.\n",
    "\n",
    "**Table #2 (Types of Disasters By State/County):** This dataset was fairly clean. We first focused on extracting the relevant columns, which were \"state\", \"declarationType\", \"incidentType\", \"declarationTitle\", and \"declarationArea\". These were the relevant variables since we want each state, the type of natural disaster, and whether it occured on a county level or not. We filtered the dataset to only contain natural disasters that occurred at the county level and standardized the column. We decided to keep the year it happened rather than the exact month and day since in our EDA in the future we would like to explore the natural disaster frequencies by decade. To make future analyses more convienient, we renamed some of the column names. Also, we checked for missing values and found that there were none. \n",
    "\n",
    "**Table #3 (National Risk Index):** This dataset was fairly clean. We focused on extracting the relevant columns, such as county, population size, National Risk Index score, and expected annual loss. These variables are important because we would like to compare counties per capita. We also decided to lowercase all the columns and replace spaces with underscores for consistency across all dataframes.\n",
    "\n",
    "**Table #4 (States with Coastline):** This dataset was fairly clean. Originally, each observation in this dataset contained a state and its associated coast. If the state did not have a coast associated, it had a value \"None\". To aid with future analyses, such as fitting multiple linear regression models later on, we changed the \"coast\" column to be binary where 0 means that a state does not have a coast associated with it and 1 means that a state does have a coast associated with it.\n",
    "\n",
    "**Table #6 (Average Climate by County)**\n",
    "This dataset was very clean. All that we needed to change was remove ID numbers after the state abbreviations and change the column names from \"Location ID\" to \"State\", \"Location\" to \"County\", and \"Value\" to \"Temperature (F)\". \n",
    "\n",
    "We plan to merge each of the datasets by either the county or state columns. Additionally, we have datasets containing the number of casualties, number of injuries, and property damage amount per state for the years 2015-2020 that we plan on processing and cleaning after the checkpoint (these datasets are in the \"datasets/nat_disast_bystate_deaths_cost‚Äù folder)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Table #1 (Frequency of Disaster By State)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_df = pd.read_csv('datasets/natural_disaster_frequencies.csv')\n",
    "\n",
    "# replace space with underscores in column names\n",
    "freq_df.columns = freq_df.columns.str.replace(' ', '_')\n",
    "\n",
    "# check for NaNs\n",
    "assert(freq_df.isna().sum().sum() == 0)\n",
    "\n",
    "freq_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Table #2 (Types of Disaster By State/County)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_county(str_in):\n",
    "    try:\n",
    "        if '(County)' in str_in:\n",
    "            output = str_in.replace('(County)','')\n",
    "        else:\n",
    "            output = None\n",
    "    except: \n",
    "        output = None\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "def standardize_year(str_in):\n",
    "    try:\n",
    "        output = str_in.split('T')[0]\n",
    "        output = pd.to_datetime(str_in).year\n",
    "    except:\n",
    "        output = None\n",
    "        \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disaster_type_df = pd.read_csv('datasets/DisasterDeclarationsSummaries.csv')\n",
    "\n",
    "# select a subset of the columns\n",
    "wanted_columns = ['state', 'declarationDate','incidentType','declarationTitle','designatedArea']\n",
    "\n",
    "# rename the columns\n",
    "disaster_type_df = disaster_type_df[wanted_columns].rename(columns={\"declarationDate\":\"year\", \"designatedArea\": \"county\", \"incidentType\":\"disaster_type\", \"declarationTitle\":\"disaster_declaration\"})\n",
    "\n",
    "# Set \"Statewide\" to None and strip \"(County)\" from all counties\n",
    "disaster_type_df['county'] = disaster_type_df['county'].apply(standardize_county)\n",
    "\n",
    "# filter dataset to only include non-null \n",
    "disaster_type_df = disaster_type_df[~disaster_type_df['county'].isnull()]\n",
    "\n",
    "# strip year column to only include year\n",
    "disaster_type_df['year'] = disaster_type_df['year'].apply(standardize_year)\n",
    "\n",
    "# sort by year\n",
    "disaster_type_df = disaster_type_df.sort_values('year').reset_index(drop = True)\n",
    "\n",
    "# check for no NaNs\n",
    "assert(disaster_type_df.isna().sum().sum() == 0)\n",
    "\n",
    "disaster_type_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Table #3 (NRI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the \"NRI Data Dictionary in the datasets/NRI_Table_Counties\" to see what the cols mean\n",
    "# EAL = \"Expected Annual Lost\", quantifies the anticipated economic damage resulting from natural hazards each year. \n",
    "# 1-100 scale\n",
    "\n",
    "df_nri = pd.read_csv('datasets/NRI_Table_Counties/NRI_Table_Counties.csv')\n",
    "\n",
    "# select a subset of the columns\n",
    "wanted_cols = ['STATE','STATEABBRV','COUNTY','POPULATION','AREA','RISK_SCORE','RISK_RATNG','EAL_SCORE','EAL_RATNG']\n",
    "df_nri = df_nri[wanted_cols]\n",
    "\n",
    "# lowercase all columns\n",
    "df_nri.columns = df_nri.columns.str.lower()\n",
    "\n",
    "# rename columns\n",
    "df_nri = df_nri.rename(columns={\"stateabbrv\":\"state_abbrv\", \"risk_ratng\":\"risk_rating\" ,\"eal_ratng\":\"eal_rating\"})\n",
    "\n",
    "# make sure there are no NaNs\n",
    "assert(df_nri.isna().sum().sum() == 0)\n",
    "\n",
    "df_nri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Table #4 (States with Coastline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coastline_df = pd.read_csv('datasets/states_with_coastline.csv')\n",
    "coastline_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check data types for each column\n",
    "coastline_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coastline_df['coast'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for missing values\n",
    "coastline_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binarize \"coast\" column. 0 = no coastline, 1 = yes coastline\n",
    "def clean_coast(coast_val):\n",
    "    coast_val = coast_val.lower()\n",
    "\n",
    "    if \"none\" in coast_val:\n",
    "        coast_val = coast_val.replace(\"none\", \"0\")\n",
    "        output = int(coast_val)\n",
    "    else:\n",
    "        output = 1\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test function from above\n",
    "assert clean_coast('None') == 0\n",
    "assert clean_coast('Atlantic Ocean') == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coastline_df['coast'] = coastline_df['coast'].apply(clean_coast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coastline_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Table #5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('datasets/cost_of_living.csv')\n",
    "state_name = ['ALABAMA', 'ALASKA', 'ARIZONA', 'ARKANSAS', 'CALIFORNIA', 'COLORADO', 'CONNECTICUT', 'DELAWARE', \n",
    "              'FLORIDA', 'GEORGIA', 'HAWAII', 'IDAHO', 'ILLINOIS', 'INDIANA', 'IOWA', 'KANSAS', 'KENTUCKY', \n",
    "              'LOUISIANA', 'MAINE', 'MARYLAND', 'MASSACHUSETTS', 'MICHIGAN', 'MINNESOTA', 'MISSISSIPPI', \n",
    "              'MISSOURI', 'MONTANA', 'NEBRASKA', 'NEVADA', 'NEW HAMPSHIRE', 'NEW JERSEY', 'NEW MEXICO', 'NEW YORK', \n",
    "              'NORTH CAROLINA', 'NORTH DAKOTA', 'OHIO', 'OKLAHOMA', 'OREGON', 'PENNSYLVANIA', 'RHODE ISLAND', \n",
    "              'SOUTH CAROLINA', 'SOUTH DAKOTA', 'TENNESSEE', 'TEXAS', 'UTAH', 'VERMONT', 'VIRGINIA', 'WASHINGTON', \n",
    "              'WEST VIRGINIA', 'WISCONSIN', 'WYOMING']\n",
    "state_abv = ['AL', 'AK', 'AZ', 'AR', 'CA', 'CO', 'CT', 'DE', 'FL', 'GA', 'HI', 'ID', 'IL', 'IN', 'IA', 'KS', 'KY', 'LA'\n",
    "             , 'ME', 'MD', 'MA', 'MI', 'MN', 'MS', 'MO', 'MT', 'NE', 'NV', 'NH', 'NJ', 'NM', 'NY', 'NC', 'ND', 'OH', \n",
    "             'OK', 'OR', 'PA', 'RI', 'SC', 'SD', 'TN', 'TX', 'UT', 'VT', 'VA', 'WA', 'WV', 'WI', 'WY']\n",
    "\n",
    "state_dict = dict(zip(state_abv,state_name))\n",
    "df = df.replace({'State':state_dict})\n",
    "geolocator = Nominatim(user_agent='find-county')\n",
    "counties = list(zip(df.City,df.State))\n",
    "counties\n",
    "new_counties = []\n",
    "for county in counties:\n",
    "    city_state = (county[0]+' '+county[1])\n",
    "    county = geolocator.geocode(city_state)[0].split(', ')[1]\n",
    "    new_counties.append(county)\n",
    "    # print(county,len(new_counties))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "444"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['County'] = new_counties\n",
    "county = df.pop('County')\n",
    "df.insert(1,'County',county)\n",
    "df.County.str.count(\"County\").sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>City</th>\n",
       "      <th>County</th>\n",
       "      <th>State</th>\n",
       "      <th>Cost of Living Index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Abilene</td>\n",
       "      <td>Taylor County</td>\n",
       "      <td>TEXAS</td>\n",
       "      <td>89.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Adrian</td>\n",
       "      <td>Lenawee County</td>\n",
       "      <td>MICHIGAN</td>\n",
       "      <td>90.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Akron</td>\n",
       "      <td>Summit County</td>\n",
       "      <td>OHIO</td>\n",
       "      <td>89.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Alamogordo</td>\n",
       "      <td>Otero County</td>\n",
       "      <td>NEW MEXICO</td>\n",
       "      <td>85.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Albany</td>\n",
       "      <td>Dougherty County</td>\n",
       "      <td>GEORGIA</td>\n",
       "      <td>87.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>505</th>\n",
       "      <td>Wheeling</td>\n",
       "      <td>1528</td>\n",
       "      <td>WEST VIRGINIA</td>\n",
       "      <td>84.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>506</th>\n",
       "      <td>New London</td>\n",
       "      <td>New London County</td>\n",
       "      <td>CONNECTICUT</td>\n",
       "      <td>105.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>507</th>\n",
       "      <td>Daphne</td>\n",
       "      <td>Baldwin County</td>\n",
       "      <td>ALABAMA</td>\n",
       "      <td>96.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>508</th>\n",
       "      <td>Victoria</td>\n",
       "      <td>Texas</td>\n",
       "      <td>TEXAS</td>\n",
       "      <td>89.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>509</th>\n",
       "      <td>Aberdeen</td>\n",
       "      <td>Grays Harbor County</td>\n",
       "      <td>WASHINGTON</td>\n",
       "      <td>97.6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>510 rows √ó 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           City               County          State  Cost of Living Index\n",
       "0       Abilene        Taylor County          TEXAS                  89.1\n",
       "1        Adrian       Lenawee County       MICHIGAN                  90.5\n",
       "2         Akron        Summit County           OHIO                  89.4\n",
       "3    Alamogordo         Otero County     NEW MEXICO                  85.8\n",
       "4        Albany     Dougherty County        GEORGIA                  87.3\n",
       "..          ...                  ...            ...                   ...\n",
       "505    Wheeling                 1528  WEST VIRGINIA                  84.1\n",
       "506  New London    New London County    CONNECTICUT                 105.9\n",
       "507      Daphne       Baldwin County        ALABAMA                  96.6\n",
       "508    Victoria                Texas          TEXAS                  89.5\n",
       "509    Aberdeen  Grays Harbor County     WASHINGTON                  97.6\n",
       "\n",
       "[510 rows x 4 columns]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Table #6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>State</th>\n",
       "      <th>County</th>\n",
       "      <th>Temperature (F)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AL</td>\n",
       "      <td>Autauga County</td>\n",
       "      <td>65.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AL</td>\n",
       "      <td>Baldwin County</td>\n",
       "      <td>68.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AL</td>\n",
       "      <td>Barbour County</td>\n",
       "      <td>66.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AL</td>\n",
       "      <td>Bibb County</td>\n",
       "      <td>63.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AL</td>\n",
       "      <td>Blount County</td>\n",
       "      <td>62.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3132</th>\n",
       "      <td>AK</td>\n",
       "      <td>Skagway Municipality</td>\n",
       "      <td>30.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3133</th>\n",
       "      <td>AK</td>\n",
       "      <td>Southeast Fairbanks Census Area</td>\n",
       "      <td>25.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3134</th>\n",
       "      <td>AK</td>\n",
       "      <td>Wrangell City and Borough</td>\n",
       "      <td>40.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3135</th>\n",
       "      <td>AK</td>\n",
       "      <td>Yakutat City and Borough</td>\n",
       "      <td>33.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3136</th>\n",
       "      <td>AK</td>\n",
       "      <td>Yukon-Koyukuk Census Area</td>\n",
       "      <td>27.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3137 rows √ó 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     State                           County  Temperature (F)\n",
       "0       AL                   Autauga County             65.7\n",
       "1       AL                   Baldwin County             68.4\n",
       "2       AL                   Barbour County             66.0\n",
       "3       AL                      Bibb County             63.9\n",
       "4       AL                    Blount County             62.7\n",
       "...    ...                              ...              ...\n",
       "3132    AK             Skagway Municipality             30.9\n",
       "3133    AK  Southeast Fairbanks Census Area             25.7\n",
       "3134    AK        Wrangell City and Borough             40.0\n",
       "3135    AK         Yakutat City and Borough             33.9\n",
       "3136    AK        Yukon-Koyukuk Census Area             27.2\n",
       "\n",
       "[3137 rows x 3 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Dataset for the mean climate for each county in the USA except those in Hawaii. The climate is measured over a 5-year span from 2017-2022. \n",
    "climate_df = pd.read_csv('datasets/Average_Climate_By_County.csv')\n",
    "climate_df['Location ID'] = climate_df['Location ID'].apply([lambda x: x[0:2]])\n",
    "climate_df = climate_df.rename(columns={'Location ID': 'State', 'Location': 'County', 'Value': 'Temperature (F)'})\n",
    "climate_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
